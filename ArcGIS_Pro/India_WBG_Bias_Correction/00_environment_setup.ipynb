{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Import necessary libraries\n",
    "In Python, most functionality comes in the form of modules, which must be imported by your script prior to use. It is common for \n",
    "most python scripts and notebooks to start with an import block, where required functions and classes may be imported.\n",
    "\n",
    "\n",
    "The import is typically in the form of `import module`, but you can also import submodules using `from module import submodule`.\n",
    "You may also import a module and give it an alternate name using `import module as new_name`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Import Python core modules\n",
    "import time\n",
    "tic = time.time()\n",
    "import os\n",
    "import sys\n",
    "import pathlib as pl\n",
    "import platform\n",
    "from pprint import pprint\n",
    "from datetime import datetime, timedelta\n",
    "import shutil\n",
    "\n",
    "# Import modules for reading archived data\n",
    "from zipfile import ZipFile\n",
    "import gzip\n",
    "\n",
    "# Import module for accessing resources on a file server\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "# Import GIS libraries\n",
    "import arcpy\n",
    "import arcgis\n",
    "from arcgis.gis import GIS\n",
    "\n",
    "# Import modules for numerical computation and analysis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from scipy import stats\n",
    "\n",
    "# Import modules for plotting and visuzliation\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import widgets\n",
    "from ipywidgets import *\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Do not display warnings. This is because the geoglows.streamflow.latlon_to_reach issues a shapely deprecation warning every time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Install and import libraries that are not part of the ArcGIS Pro Python environment\n",
    "\n",
    "These are Python libraries that are not distributed with this distribution of ArcGIS Pro.\n",
    "\n",
    "* `geoglows` library is used to pull streamflow forecasts from the GEOGloWS service.\n",
    "* `plotly` library is used in conjunction with the `geoglows` library to enable interactive plotting.\n",
    "* `gdown` library will allow us to download files from a Google Drive dataset.\n",
    "* `cartopy` library enables some custom geospatial plotting that is required by the Q2Q scripts provided. We will attempt to import the library and then install using a `conda` command if the library is not available in the current environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not import geoglows library. Installing using pip.\n",
      "Could not import gdown library. Installing using conda.\n",
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "﻿\n",
      "## Package Plan ##\n",
      "﻿\n",
      "  environment location: C:\\Users\\ksampson\\AppData\\Local\\ESRI\\conda\\envs\\arcgispro-py3-clone\n",
      "﻿\n",
      "  added / updated specs:\n",
      "    - conda-forge::gdown\n",
      "    - esri::numpy=1.20\n",
      "﻿\n",
      "﻿\n",
      "The following packages will be downloaded:\n",
      "﻿\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    backcall-0.2.0             |     pyhd3eb1b0_0          14 KB  esri\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:          14 KB\n",
      "﻿\n",
      "The following NEW packages will be INSTALLED:\n",
      "﻿\n",
      "  filelock           pkgs/main/win-64::filelock-3.9.0-py39haa95532_0\n",
      "  gdown              conda-forge/noarch::gdown-4.7.1-pyhd8ed1ab_0\n",
      "﻿\n",
      "The following packages will be UPDATED:\n",
      "﻿\n",
      "  attrs              pkgs/main/noarch::attrs-21.4.0-pyhd3e~ --> pkgs/main/win-64::attrs-23.1.0-py39haa95532_0\n",
      "  beautifulsoup4                      4.11.1-py39haa95532_0 --> 4.12.2-py39haa95532_0\n",
      "  certifi                          2022.9.24-py39haa95532_0 --> 2023.7.22-py39haa95532_0\n",
      "  cffi                                1.15.1-py39h2bbff1b_0 --> 1.15.1-py39h2bbff1b_3\n",
      "  colorama                             0.4.5-py39haa95532_0 --> 0.4.6-py39haa95532_0\n",
      "  cryptography                                38.0.4-py39_0 --> 41.0.3-py39_2\n",
      "  debugpy                              1.5.1-py39hd77b12b_0 --> 1.6.7-py39hd77b12b_0\n",
      "  importlib-metadata                  4.11.3-py39haa95532_0 --> 6.0.0-py39haa95532_0\n",
      "  importlib_metadata                      4.11.3-hd3eb1b0_0 --> 6.0.0-hd3eb1b0_0\n",
      "  jedi                      esri/win-64::jedi-0.18.2-py39_0 --> esri/noarch::jedi-0.18.2-py_1\n",
      "  jsonschema                          4.16.0-py39haa95532_0 --> 4.17.3-py39haa95532_0\n",
      "  jupyter_core                                4.11.2-py39_0 --> 4.11.2-py39_1\n",
      "  libsodium                                        1.0.18-4 --> 1.0.18-5\n",
      "  libxml2                                2.10.3-arcgispro_0 --> 2.10.4-arcgispro_0\n",
      "  nest-asyncio                         1.5.5-py39haa95532_0 --> 1.5.6-py39haa95532_0\n",
      "  openssl                                           3.0.7-0 --> 3.0.10-0\n",
      "  packaging          pkgs/main/noarch::packaging-21.3-pyhd~ --> pkgs/main/win-64::packaging-23.1-py39haa95532_0\n",
      "  pip                                 22.2.2-py39haa95532_0 --> 23.3-py39haa95532_0\n",
      "  platformdirs                         2.5.2-py39haa95532_0 --> 3.10.0-py39haa95532_0\n",
      "  pygments                                       2.9.0-py_0 --> 2.14.0-py_0\n",
      "  pyopenssl          pkgs/main/noarch::pyopenssl-22.0.0-py~ --> pkgs/main/win-64::pyopenssl-23.2.0-py39haa95532_0\n",
      "  pyzmq                                       24.0.1-py39_1 --> 25.0.2-py39_0\n",
      "  requests                            2.28.1-py39haa95532_0 --> 2.31.0-py39haa95532_0\n",
      "  setuptools                                  65.5.1-py39_0 --> 67.7.2-py39_0\n",
      "  soupsieve                      2.3.2.post1-py39haa95532_0 --> 2.5-py39haa95532_0\n",
      "  sqlite                                           3.40.1-0 --> 3.41.2-0\n",
      "  terminado                           0.13.1-py39haa95532_0 --> 0.17.1-py39haa95532_0\n",
      "  tqdm                                4.64.1-py39haa95532_0 --> 4.65.0-py39hd4e2768_0\n",
      "  typing-extensions                    4.3.0-py39haa95532_0 --> 4.7.1-py39haa95532_0\n",
      "  typing_extensions                    4.3.0-py39haa95532_0 --> 4.7.1-py39haa95532_0\n",
      "  tzdata                                   2022e-h04d1e81_0 --> 2023c-h04d1e81_0\n",
      "  urllib3                            1.26.12-py39haa95532_0 --> 1.26.16-py39haa95532_0\n",
      "  wheel              pkgs/main/noarch::wheel-0.37.1-pyhd3e~ --> pkgs/main/win-64::wheel-0.41.2-py39haa95532_0\n",
      "  xz                                       5.2.8-h8cc25b3_0 --> 5.4.2-h8cc25b3_0\n",
      "  zeromq                                            4.3.4-0 --> 4.3.4-1\n",
      "  zipp                                 3.8.0-py39haa95532_0 --> 3.11.0-py39haa95532_0\n",
      "﻿\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "﻿\n",
      "  backcall                                        pkgs/main --> esri\n",
      "﻿\n",
      "﻿\n",
      "﻿\n",
      "Downloading and Extracting Packages\n",
      "﻿\n",
      "backcall-0.2.0       | 14 KB     |            |   0% \n",
      "backcall-0.2.0       | 14 KB     | ########## | 100% \n",
      "backcall-0.2.0       | 14 KB     | ########## | 100% \n",
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... done\n",
      "Retrieving notices: ...working... done\n"
     ]
    }
   ],
   "source": [
    "# Geoglows API\n",
    "try:\n",
    "    import geoglows\n",
    "except ImportError:\n",
    "    print('Could not import geoglows library. Installing using pip.')\n",
    "    \n",
    "    # Start by installing the package and importing it to your code. Run this cell to do that.\n",
    "    !pip install geoglows -q\n",
    "    import geoglows\n",
    "\n",
    "# Plotly plotting library to support geoglows plotting\n",
    "try:\n",
    "    import plotly\n",
    "except ImportError:\n",
    "    print('Could not import plotly library. Installing using pip.')\n",
    "    \n",
    "    # Start by installing the package and importing it to your code. Run this cell to do that.\n",
    "    !pip install -U kaleido plotly==5.3.1 -q\n",
    "    import plotly\n",
    "\n",
    "# In the Esri Notebook environment, plotly only seems to be able to render to the browser\n",
    "plotly.io.renderers.default = \"browser\"\n",
    "\n",
    "# GDOWN - for downloading files from Google Drive\n",
    "try:\n",
    "    import gdown\n",
    "except ImportError:\n",
    "    print('Could not import gdown library. Installing using conda.')\n",
    "    !conda install --freeze-installed esri::numpy=1.20.* conda-forge::gdown -y\n",
    "    import gdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Connecting ArcGIS Online session through ArcGIS Pro\n",
    "\n",
    "This will establish the user connection to ArcGIS Online. There are many ways to establish this connection. [Read here for all possible authentication methods](https://developers.arcgis.com/python/guide/working-with-different-authentication-schemes/)\n",
    "\n",
    "It is often useful to write scripts that work against the active portal in the ArcGIS Pro app.\n",
    "\n",
    "Using the `pro` authentication scheme, scripts can create an instance of the `GIS` class representing the active portal in ArcGIS Pro without requiring the user to pass their credentials. In this mode, users can leverage the Pro app to login to the portal and their scripts can use whichever Portal is currently active. This mode can also serve as a bridge for users with advanced authentication scenarios like IWA using NTLM or Kerberos or Smart Card where signing in with credentials may not be possible or desirable.\n",
    "\n",
    "> NOTE: When using a [Named User license](https://pro.arcgis.com/en/pro-app/get-started/licensing-arcgis-pro.htm) to license ArcGIS Pro, unless the [Sign me in automatically](https://pro.arcgis.com/en/pro-app/help/projects/sign-in-to-your-organization.htm) checkbox is selected when signing into the licensing portal or ArcGIS Pro has been [licensed for offline use](https://pro.arcgis.com/en/pro-app/get-started/start-arcgis-pro-with-a-named-user-license.htm#ESRI_SECTION1_15AD453E27C446CE9B51D45C021E8067), ArcGIS Pro should be installed and concurrently running on the machine that executes the script for authentication to succeed. When selecting `Sign me in automatically`, ArcGIS \n",
    "> Pro can remain closed for 2 weeks by default before the portal token expires. See [here](https://enterprise.arcgis.com/en/portal/latest/administer/windows/specify-the-default-token-expiration-time.htm) for details on configuring token settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Active Portal in ArcGIS Pro\n",
      "Logged in as ksampson\n"
     ]
    }
   ],
   "source": [
    "print(\"Active Portal in ArcGIS Pro\")  \n",
    "gis = GIS(\"pro\")\n",
    "print(\"Logged in as \" + str(gis.properties.user.username))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Get information about this map document\n",
    "\n",
    "Using `arcpy.mp`, we can manipulate the content of existing ArcGIS Pro projects. We will read the information about this project in order to set some important path variables for using later in the training. [Read more...](https://pro.arcgis.com/en/pro-app/latest/arcpy/mapping/introduction-to-arcpy-mp.htm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Get available information from the current map document\n",
    "aprx = arcpy.mp.ArcGISProject(\"current\")\n",
    "\n",
    "# Find the directory of the current project\n",
    "aprx_path = aprx.filePath\n",
    "\n",
    "# Find the home folder for the current project\n",
    "home_folder = aprx.homeFolder\n",
    "\n",
    "# Get a list of the current folder connections available to the current project\n",
    "folder_connections = aprx.folderConnections\n",
    "\n",
    "# Get the default GeoDatabase associated with the current project\n",
    "default_gdb = aprx.defaultGeodatabase\n",
    "\n",
    "# Get names for open Maps in this project\n",
    "open_maps = [item.name for item in aprx.listMaps()]\n",
    "\n",
    "# Move system path to the home folder\n",
    "sys.path.insert(0, home_folder)\n",
    "os.chdir(home_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Find local environment and directory structure\n",
    "\n",
    "This function use the .aprx home folder location, and determine other relative paths. The rest of the training will assume that the directory structure has not been altered from that found in the GitHub repository, and that there are separate directories for `\\notebooks`, `\\scripts`, and `\\data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Derive the root folder for this training based on the .aprx home folder\n",
    "# Assumes map document project is in the GitHub repository, and moves up 2 directories\n",
    "root_dir = pl.Path(home_folder).parent.parent\n",
    "\n",
    "# Set the notebook directory\n",
    "notebook_dir = root_dir / 'notebooks'\n",
    "\n",
    "# Set the directory to the scripts used in the training\n",
    "scripts_dir = root_dir / 'scripts'\n",
    "\n",
    "# Set the directory to the datasets used in the training\n",
    "data_dir = root_dir / 'data'\n",
    "input_data_dir = data_dir / 'input'\n",
    "\n",
    "# Set the directory to save output datasets derived during the training\n",
    "output_data_dir = data_dir / 'output'\n",
    "\n",
    "# User specific data\n",
    "user_name = os.environ.get('USERNAME', None)\n",
    "temp_dir = os.environ.get('TEMP', fr'C:\\{user_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The next codeblock will attempt to find the current active conda environment and path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current conda environment:\n",
      "\tarcgispro-py3-clone C:\\Users\\ksampson\\AppData\\Local\\ESRI\\conda\\envs\\arcgispro-py3-clone\n"
     ]
    }
   ],
   "source": [
    "# Get the conda path and environment name\n",
    "envs = ! conda env list\n",
    "active_env = list(filter(lambda s: '*' in str(s), envs))[0]\n",
    "env_name = active_env.split()[0]\n",
    "active_env_dir = pl.Path(active_env.split()[-1])\n",
    "print('Current conda environment:\\n\\t{0} {1}'.format(env_name, active_env_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Output Options\n",
    "\n",
    "To ensure the default behaviour in ArcGIS Pro to automatically add geoprocessing outputs to an active map or scene is enabled, and to ensure you are allowed to overwrite existing outputs, the following properties of the `arcpy.env` module may be set to `True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "arcpy.env.addOutputsToMap = True\n",
    "arcpy.env.overwriteOutput = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Download input datasets\n",
    "\n",
    "This training relies on pre-built dataset that will allow us to perform operations such as bias-correction, visualization of gridded precipitation data, and other geospatial and static tabular data. We will use the `gdown` library to pull datasets from Google Drive links that are publicaly available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found input data directory: C:\\Users\\ksampson\\Documents\\GitHub\\GloFAS_Q2Q_Bias_Correction_and_Verification\\data\\input\n"
     ]
    }
   ],
   "source": [
    "# Publicaly accessible path to the Google Drive dataset containing training data\n",
    "gfile = 'https://drive.google.com/uc?id=1GweIOjbFvPvfe88_Fuyf5WdW_Jzbx8nQ'  \n",
    "gzfilename = 'training_data.tar.gz'\n",
    "training_data_file = os.path.join(data_dir, gzfilename)\n",
    "\n",
    "if os.path.exists(input_data_dir):\n",
    "    print('Found input data directory: {0}'.format(input_data_dir))\n",
    "else:\n",
    "    os.makedirs(input_data_dir)\n",
    "    training_data_file = os.path.join(data_dir, gzfilename)\n",
    "    if not os.path.exists(training_data_file):\n",
    "        print('Downloading from Google Drive, link = {0}'.format(gfile))\n",
    "        !gdown -O {training_data_file} {gfile}\n",
    "    else:\n",
    "        print('Found GZIP archive of training data: {0}'.format(training_data_file))\n",
    "    print('    Unzipping...')\n",
    "    shutil.unpack_archive(training_data_file, extract_dir=input_data_dir, format=\"gztar\")\n",
    "    print('Unzipped archive of training data to {0}'.format(input_data_dir)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Define Functions\n",
    "\n",
    "To clean up and de-clutter the notebooks in this training, we will store various functions related to analysis and plotting here.\n",
    "\n",
    "These functions will be loaded into the namespace of each of the subsequent Esri Notebooks when `%run 00_environment_setup.ipynb` is called"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Quantile-to-Quantile mapping function, vectorized\n",
    "# https://staff.ral.ucar.edu/hopson/GloFAS/Q2Qbiascorrection/q2q_glofas.py\n",
    "def q2q(modclim, obsclim, fcst, return_pct=False):\n",
    "    #Python code to map a GLOFAS forecast to the observed value of a similar quantile\n",
    "    #Requires historical records of the model and observations to do the mapping\n",
    "    #New forecasts larger and smaller than the archived forecast\n",
    "    #distrubution will be mapped to the largest/smallest values of the observations\n",
    "    \n",
    "    # Handle the type by converting to array\n",
    "    if type(fcst) in [float, int]:\n",
    "        fcst = np.array([fcst])\n",
    "        \n",
    "    first_index=np.searchsorted(np.sort(modclim),fcst,side='left')\n",
    "    last_index=np.searchsorted(np.sort(modclim),fcst,side='right')\n",
    "    indval=np.array([np.random.choice(np.arange(first_index[n],last_index[n]+1),1)[0] for n in range(fcst.shape[0])])\n",
    "    percentile=100*((indval)/(float(len(modclim))))\n",
    "    q2q_corrected=np.percentile(obsclim,percentile)\n",
    "    if return_pct:\n",
    "        return q2q_corrected, percentile/100.\n",
    "    else:\n",
    "        return q2q_corrected\n",
    "\n",
    "# Add to an existing dataframe, a cumulative density function based on a column.\n",
    "def get_cdf(in_df, column=''):\n",
    "    '''\n",
    "    Get the frequency, PDF and CDF for each value in the series. \n",
    "    Input is a pandas DataFrame\n",
    "    ''' \n",
    "    \n",
    "    # Calculate the percentile of each value in the input array\n",
    "    in_df['cdf'] = np.array([stats.percentileofscore(in_df[column], item)/100. for item in in_df[column]])\n",
    "    return in_df\n",
    "    \n",
    "# Function to wrap a dropdown widget with value list\n",
    "def drop_down_select(value_list, descriptor='Select:'):\n",
    "    # Initialize the dropdown widget\n",
    "    w = widgets.Dropdown(\n",
    "        options=value_list,\n",
    "        description=descriptor,\n",
    "        disabled=False)\n",
    "\n",
    "    # Set an observe object to detect any changes\n",
    "    w.observe(lambda c: plot_content(c['new']) if (c['type'] == 'change' and c['name'] == 'value') else None)\n",
    "    return w\n",
    "\n",
    "def get_size_gb(input_da, silent=False):\n",
    "    '''\n",
    "    Get the size of an Xarray DataSet or DataArray and print total size\n",
    "    '''\n",
    "    # Print out information about the input dataset\n",
    "    dataset_size_GB = input_da.nbytes/(1024.**3)\n",
    "    if not silent:\n",
    "        print('Size of input dataset:\\t{0:3.2f} Gb'.format(dataset_size_GB))\n",
    "    return dataset_size_GB\n",
    "\n",
    "# Function to describe the structure of the xarray DataSet\n",
    "def report_structure(ds, variable, time_coord='time', silent=False):\n",
    "    '''\n",
    "    Inputs:\n",
    "        ds - an xarray DataSet object.\n",
    "        variable - String - a variable in the input DataSet object to examine.\n",
    "        time_coord - string - the name of the time coordinate in the input DataSet object.\n",
    "    Outputs:\n",
    "        ds - The xarray DataSet object, possibly altered to unify chunk sizes\n",
    "        timesteps - The time values in the input ifle\n",
    "    '''\n",
    "\n",
    "    # Pull the timesteps from the input file\n",
    "    timesteps = ds[time_coord].values\n",
    "    if not silent:\n",
    "        print('Found {0} timestep(s) in input file'.format(timesteps.shape[0]))\n",
    "\n",
    "    # Print out information about the input dataset\n",
    "    dataset_size_GB = get_size_gb(ds, silent=True)\n",
    "    if not silent:\n",
    "        print('Size of input dataset:  {0:3.2f} Gb'.format(dataset_size_GB))\n",
    "\n",
    "    # Find out the size of one timestep (the unit of processing)\n",
    "    timestep_size_GB = get_size_gb(ds[variable].isel({time_coord:0}), silent=True)\n",
    "    if not silent:\n",
    "        print('Size of 1 timesteps in dataset:  {0:3.3f} Gb'.format(timestep_size_GB))\n",
    "    return ds, timesteps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed importing and/or installing libraries in 255.67 seconds.\n"
     ]
    }
   ],
   "source": [
    "print('Completed importing and/or installing libraries in {0:3.2f} seconds.'.format(time.time()-tic))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ArcGISPro",
   "language": "Python",
   "name": "python3"
  },
  "language_info": {
   "file_extension": ".py",
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
