{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Gridded Precipiation Data\n",
    "\n",
    "This Esri Notebook will examine GFS gridded precipiation data. We will pull the 10 day GFS forecast from NOAA OPeNDAP servers. The gridded forecast data can be explored in this notebook using the `xarray` python library and and imported into ArcGIS Pro as a [Multidimensional Raster Layer](https://pro.arcgis.com/en/pro-app/latest/help/data/imagery/an-overview-of-multidimensional-raster-data.htm).\n",
    "\n",
    "\n",
    "1. Download GFS gridded 1-hourly and 3-hourly forecast data for the current time period.\n",
    "2. Explore the gridded forecast data spatially.\n",
    "3. Extract a time-series at 1 point within the forecast region.\n",
    "4. Perform a temporal aggregation to determine daily precipitation accumulations.\n",
    "5. Save the output to a multidimensional file format (netCDF).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the directory of the current project and add to PATH\n",
    "import sys, os, arcpy\n",
    "home_folder = arcpy.mp.ArcGISProject(\"current\").homeFolder\n",
    "sys.path.insert(0, home_folder)\n",
    "os.chdir(home_folder)\n",
    "\n",
    "# The 00_environment_setup notebook contains libraries and other things common to all the notebooks (e.g. file paths)\n",
    "%run \"00_environment_setup.ipynb\"\n",
    "\n",
    "# Additional libraries needed for this script\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set some parameters for this exercise\n",
    "\n",
    "\n",
    "#### First, set an appropriate color ramp.\n",
    "\n",
    "Here, the `gist_ncar` color ramp contains a high degree of color variation, which can be helpful for plotting precipiation amounts. There are [many color ramps](https://matplotlib.org/stable/users/explain/colors/colormaps.html) to choose from in matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precipitation colorbar choices\n",
    "cmap = matplotlib.colormaps['gist_ncar']  # Color ramp suitable for precipiation\n",
    "cmap.set_under('lightgrey')               # Set values of zero precipitation to white"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multidimensional Data\n",
    "\n",
    "Many gridded forecast products are typically produced and distributed in a multidimensional data format. These formats were designed to store data containing multiple dimensions, such as time, latitude, longitude, vertical dimensions, and others. Some popular multidimensional data formats are netCDF, GRIB, and Zarr. Some gridded forecast data products are staged in data servers and exposed for download in various formats and protocols. One such protocol is OPEeNDAP, which enables selective data retrieval via a web service. ArcGIS Pro comes installed with two python libraries that are useful for examining multidimensional data: `netCDF4-Python` and `xarray`. The `xarray` library gives users the ability to read, write, examine, plot, and process multidimensional data from a variety of formats. \n",
    "\n",
    "We will use this library extensively in this excercise to examine the latest forecast from the [NOAA Global Forecast System (GFS)](https://www.ncei.noaa.gov/products/weather-climate-models/global-forecast).\n",
    "\n",
    "During the environment setup (`%run \"00_environment_setup.ipynb\"`), we imported the xarray library using syntax `import xarray as xr`, which gives allows us to interact with this library using the shortened `xr` variable name. \n",
    "\n",
    "In this excercise, we will be using `xarray` to open multidimensional GFS forecast data using an OPeNDAP URL, which is a path to a dataset on a [data server](https://nomads.ncep.noaa.gov/info.php?page=overview) that can be accessed using the OPeNDAP protocol. This data can be queried as though it is a file on local disk. After requesting the data using `xr.open_dataset` and a valid URL, we will have access to the resulting xarray `DataSet` object. A `DataSet` is a fundamental object in `xarray`, and can store one or more variables, or `DataArrays`, along multiple dimensions. Below is an image illustrating the data concepts of the xarray library for two variables (temperature and precipitation) along dimensions of latitude and longitude and over time.\n",
    "\n",
    "![alternatvie text](https://docs.xarray.dev/en/latest/_images/dataset-diagram.png)\n",
    "\n",
    "In this analysis, we will only be examining the precipitation variable `apcpsfc` which represents accumulated precipitation at the surface in the forecast. Once the data is loaded into an xarray DataSet, we can subset in time or space, manipulate using built-in functions, and plot results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select whether or not to load precipitation data from local disk or from NOAA's servers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch to load from local disk or from NOAA via OPeNDAP\n",
    "load_nomads = False          # Load using OPeNDAP from NOAA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull GFS data directly from NCEP via OPeNDAP\n",
    "\n",
    "The [NOAA Global Forecast System (GFS)](https://www.ncei.noaa.gov/products/weather-climate-models/global-forecast) is a global numerical weather prediction system horizontal grid spacing of ~0.25 degrees. The model is run deterministically 4x per day: 00z, 06z, 12z, 18z.\n",
    "\n",
    "You can find a guide on downloading data from NOAA NCEP servers, [here](https://twister.caps.ou.edu/METR3334/GFS_data/DownloadingModelDataFromNcepServer.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_nomads:\n",
    "    # Gather the exact time representation for each of yesterday's GFS forecasts (0Z, 6Z, 12Z, 18Z)\n",
    "    fcsthrs = [0,6,12,18]\n",
    "\n",
    "    # Gather list of yesterday's forecasts from GFS\n",
    "    latest_times = [datetime.now().replace(hour=fh,minute=0,second=0,microsecond=0) - timedelta(days=1) for fh in fcsthrs]\n",
    "    date_strings = [fcst.strftime('%Y%m%d') for fcst in latest_times]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the request string based on selected model run time and desired variable\n",
    "\n",
    "We will select one of yesterday's model runs, because all model runs for today's date may not yet be complete. The next codeblock will build an OPeNDAP request URL that can be used to pull the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the variable name we will use from GFS\n",
    "varname = \"apcpsfc\"\n",
    "\n",
    "if load_nomads:\n",
    "    # Specific request parameters\n",
    "    fcsthr = fcsthrs[0]                 # 0z\n",
    "    date_string = date_strings[0]       # 0z date string     \n",
    "\n",
    "    # Define server information\n",
    "    request_template_1hr = r\"http://nomads.ncep.noaa.gov:80/dods/gfs_0p25_1hr/gfs{date_string}/gfs_0p25_1hr_{fcsthr:02d}z\"  \n",
    "    request_template_3hr = r\"https://nomads.ncep.noaa.gov/dods/gfs_0p25/gfs{date_string}/gfs_0p25_{fcsthr:02d}z\"            \n",
    "\n",
    "    # Build request URLs\n",
    "    request_url_1hr = request_template_1hr.format(date_string=date_string, fcsthr=fcsthr) #, varname=varname\n",
    "    request_url_3hr = request_template_3hr.format(date_string=date_string, fcsthr=fcsthr) #, varname=varname\n",
    "    print(request_url_1hr)\n",
    "    print(request_url_3hr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open the GFS forecast datasets (1-hourly and 3-hourly), subset in space, and combine into one `xarray` DataSet\n",
    "\n",
    "Occasionally, the URLs constructed above will not work. If an error occurs during the execution of the following codeblock, change the `fcsthr` string, and re-run both the cell above and below until you see a valid xarray DataSet object appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "if load_nomads:\n",
    "    \n",
    "    # Spatial subset parameters\n",
    "    lon_slice = slice(66.5,100.0)\n",
    "    lat_slice = slice(6.5,38.5)\n",
    "    \n",
    "    # Open OpenDAP dataset for 1 hourly data\n",
    "    dataset1 = xr.open_dataset(request_url_1hr, decode_cf=False)[varname].to_dataset()\n",
    "    dataset1 = dataset1.sel(lon=lon_slice, lat=lat_slice)   # Subset array in space first\n",
    "    dataset1 = xr.decode_cf(dataset1)                                     # Convert to CF-compliance (if possible)\n",
    "    dataset1 = dataset1.isel({'time':slice(1,121,1)})                     # Only take hours 1-120\n",
    "\n",
    "    # Open OpenDAP dataset for 1 hourly data\n",
    "    dataset2 = xr.open_dataset(request_url_3hr, decode_cf=False)[varname].to_dataset()\n",
    "    dataset2 = dataset2.sel(lon=lon_slice, lat=lat_slice)   # Subset array in space first\n",
    "    dataset2 = xr.decode_cf(dataset2)                                     # Convert to CF-compliance (if possible)\n",
    "    dataset2 = dataset2.isel({'time':slice(41,81,1)})                     # Only take hours 123-240\n",
    "\n",
    "    # Combine 1hrly for hours 0-120 and 3 hourly until hour\n",
    "    dataset_gfs = xr.concat([dataset1, dataset2], dim='time')\n",
    "\n",
    "    # Clean up\n",
    "    del dataset1, dataset2\n",
    "\n",
    "else:\n",
    "    # Load a GFS precipitation file from disk\n",
    "    in_filename = 'gfs_0p25_1hr_00z_gfs20231018_in.nc'\n",
    "    in_file = os.path.join(input_data_dir, 'precip', in_filename)\n",
    "    dataset_gfs = xr.open_dataset(in_file) #, decode_cf=True, decode_times=True)\n",
    "\n",
    "# Display the dataset\n",
    "dataset_size_GB = get_size_gb(dataset_gfs)\n",
    "dataset_gfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `xarray` library has some great tools for interacting with DataSet objects in Jupyter Notebook. If you call the DataSet object, Jupyter will render a graphical example of the DataSet. You can expand the list of `Dimensions`, `Coordinates`, `Data variables`, and `Attributes`. If metadata exists, you can expand to view metadata or a sample of the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot one timestep from the resulting dataset\n",
    "\n",
    "We have requested only one variable from the GFS forecast: `apcpsfc` or \"surface total precipitation [kg/m^2]\". This data is multidimensional, meaning it contains dimensions for `time`, as well as 2 dimensions to describe the data across space: `lat`, and `lon`. We can create a static plot of the data for any period of time available in this DataSet. Because this variable is already a total accumulated precipitation, the last timestep will give the forecasted total accumulated precipitation at the end of the simulation. Xarray gives the `.isel` method for selecting just one timestep using an index value. We can chain together the variable selection, time selection, and plotting in one command. Xarray will select a default colormap and label the axes using the coordinate labels, automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Plot the last timestep\n",
    "dataset_gfs[varname].isel({'time':-1}).plot(figsize=(10,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract a time-series from a single point within the GFS domain\n",
    "\n",
    "In addition to examining the data in space, we can also examine the data over the `time` dimension. For simplicity, we will select one grid-cell from within the model subset domain (India), and plot the data for that cell over time. Again, `xarray` will know how to plot the data based on the number of dimensions (in this case just `time`), and plot the data with a line, labeling the axes with the coordinate values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Select a location in the model's x,y coordinate system.\n",
    "point_lon_lat = (76.586, 12.43)    # KRS Dam\n",
    "print(f'Extracting point time-series for point with (longitude,latitude) = ({point_lon_lat})')\n",
    "\n",
    "# Use nearest neighbor method to extract the grid cell closest to the desired point\n",
    "ds_point = dataset_gfs.sel(lat=point_lon_lat[1], lon=point_lon_lat[0], method=\"nearest\")\n",
    "print('Nearest grid point to {0} is ({1},{2})'.format(point_lon_lat, \n",
    "                                                      float(ds_point['lon'].data), \n",
    "                                                      float(ds_point['lat'].data)))\n",
    "\n",
    "# Plot the time-series\n",
    "ds_point[varname].plot(figsize=(20,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot precipitation rate for each timestep using bars\n",
    "\n",
    "Note from the previous plot that the data represented by the variable `apcpsfc` is accumulated over time, and does not represent the precipitation at each timestep. We can easily calculate the time-step accumulation by differencing the data over the `time` dimension, using the `xarray` `.diff` method, and specifying the dimension over which to perform the operation. Once we calculate the precipitation over each time-step, this will give use a better sense of the precipitation intensity for this location over the model run. \n",
    "\n",
    "We will add some `matplotlib` plotting options to customize this plot. Remember that the default plot in xarray for time-series data is a line. If we wish to modify this plot to be a bar plot, we can take control of the plotting and setup `matplotlib` plot objects (`fig`, `ax`) and pass the axes to `xarray` to populate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# De-accumulate from accumulated precipitation to timestep precip\n",
    "precip_hourly = ds_point[varname].diff(dim='time')\n",
    "precip_hourly.name = 'Surface Precipitation Rate [kg/m^2]'\n",
    "\n",
    "# Timesteps have variable length in this dataframe. Normalize to accumulated precip per hour?\n",
    "timestep_length_hrs = precip_hourly['time'].diff(dim='time') / pd.Timedelta(hours=1)\n",
    "precip_hourly = precip_hourly / timestep_length_hrs\n",
    "    \n",
    "# For more control over the plot style, convert to pandas and use pyplot funcitons\n",
    "fig, ax = plt.subplots(figsize=(20,6))\n",
    "rain_df = precip_hourly.to_dataframe(name='pr').drop(columns=['lat', 'lon'])\n",
    "rain_df = rain_df.reset_index()\n",
    "rain_df = rain_df.set_index(rain_df['time'].dt.strftime('%b-%d-%y'))\n",
    "ax = rain_df.plot.bar(ax=ax, x='time', width=1)\n",
    "ax.legend(loc=1)  \n",
    "plt.locator_params(axis='x', nbins=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate a daily 2D precipitation totals for each forecast day\n",
    "\n",
    "Another way we may wish to examine this data is to resample the time-period and look at the spatial distribution of accumulated precipitation over each day in the forecast. We can easily do this by de-accumulating the `apcpsfc` variable as we did before, but while preserving the 2D spatial components of the original data. Again, we will perform the difference over time using the `.diff` function within `xarray`, specifying `time` as the dimension to perform the operation over. The result of this operation is a DataArray, since we specify only one variable as input to the `diff` operation. Because the precipitation data is no longer accumulated, we will change the variable name to `pr` (precipitation rate). Then, we will resample to daily using the time-aggregation string `\"1D\"` in the [`.resample`](https://docs.xarray.dev/en/stable/generated/xarray.Dataset.resample.html) function, and specify that we wish to compute the sum over each new time-period. Finally, we will convert the data back into a DataSet (from a DataArray). The result will be a DataSet of daily total precipitation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time                     \n",
    "\n",
    "# Isolate the precipitation in the first timestep\n",
    "t1_precip = dataset_gfs[varname].isel({'time':0})     \n",
    "\n",
    "# Calculate difference over time to de-accumulate precipitation\n",
    "precip_hourly_2d = dataset_gfs[varname].diff(dim='time', label='upper') \n",
    "\n",
    "# Clip off the final time-step since it will be labeled with the following day (0Z)\n",
    "precip_hourly_2d = precip_hourly_2d.isel({'time':slice(None, -1, None)})\n",
    "\n",
    "# Concatenate first time-step precip with the de-accumulated precip for all subseqent timesteps\n",
    "precip_hourly_2d = xr.concat([t1_precip, precip_hourly_2d], dim='time')\n",
    "\n",
    "# Rename the DataArray name\n",
    "precip_hourly_2d.name = 'pr'\n",
    "\n",
    "# Calculate the sum of precip for each day\n",
    "daily_precipitation = precip_hourly_2d.resample(time=\"1D\").sum().to_dataset()\n",
    "daily_precipitation['pr']/=24.                                      # Ensure units are in mm/h\n",
    "daily_precipitation['pr'].attrs = precip_hourly_2d.attrs            # Add variable attributes\n",
    "daily_precipitation['pr'].attrs['units'] = 'mm/hr'                  # Change units to account for daily total precip\n",
    "daily_precipitation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the 2D precipitation totals for each forecast day\n",
    "\n",
    "#### Plot each forecast day as a separate `facet` in the resulting plot\n",
    "\n",
    "`Xarray` has many [plotting options](https://docs.xarray.dev/en/latest/user-guide/plotting.html). One is the ability to show a gridded array as an image (`.imshow`) and we can create a faceted plot with multiple images. In this case, we want to visualize the daily accumulated precipitation for each day in the GFS forecast. The result of the previous resampling operation will be a dataset of 10 or 11 forecast days, depending on the initialization time of the forecast. Thus, we will want the faceted plot to show 1 week of data on each row, and we will secify that it should wrap the columns each 7 values (`col_wrap=7`). Further, we will specify the precipitation color-ramp that we defined earlier and set a minimum valid value to plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the plot into one week per row\n",
    "facets = daily_precipitation['pr'].plot.imshow(x=\"lon\", y=\"lat\", col='time', col_wrap=7, cmap=cmap, vmin=0.1)\n",
    "\n",
    "# Plot options to remove x and y labels, but add coordinates as title\n",
    "facets.set_xlabels('')\n",
    "facets.set_ylabels('')\n",
    "facets.set_axis_labels('')\n",
    "facets.set_titles()        # This will set a title for each plot likt '{coord} = {value}'\n",
    "\n",
    "# Fix some remaining issues at the axis level\n",
    "for i, ax in enumerate(facets.axes.flat):       \n",
    "    #ax.set_title(\"Forecast Day {0}\".format(i+1))\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xticks([], minor=True)\n",
    "    ax.set_yticks([], minor=True)\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_xticklabels([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the GFS forecast data to local disk\n",
    "\n",
    "In order to bring gridded multidimensional data into ArcGIS Pro, we must convert it to a file-based data format. The Network Common Data Format (netCDF) is used widely in the atmospheric sciences for multidimensional data. Here, we will use the xarray command `.to_netcdf` on the xarray DataSet object to write the full GFS forecast time-series to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Save the file to disk using info about the forecast model and initialization time\n",
    "if load_nomads:\n",
    "    model_info = os.path.basename(request_url_1hr)\n",
    "    time_base = os.path.basename(os.path.dirname(request_url_1hr))\n",
    "else:\n",
    "    # Gather info from the input filename\n",
    "    model_info = in_filename.split('_gfs')[0]\n",
    "    time_base = in_filename.split('z_')[1].split('.nc')[0]\n",
    "    time_base = time_base.split('_in')[0]\n",
    "\n",
    "# Flip the dimensions - This is critical for Esri to understand the data\n",
    "precip_hourly_2d = precip_hourly_2d.transpose(\"time\", \"lat\", \"lon\")\n",
    "daily_precipitation = daily_precipitation.transpose(\"time\", \"lat\", \"lon\")\n",
    "\n",
    "# Save the hourly processed data\n",
    "out_basename = '{0}_{1}'.format(model_info, time_base)\n",
    "out_file = os.path.join(output_data_dir, out_basename+'.nc')\n",
    "print('Writing forecast data to file: {0}'.format(out_file))\n",
    "precip_hourly_2d.to_netcdf(out_file, encoding={\"time\": {\"dtype\": \"float64\"}}) #, 'pr':{'_FillValue':'9.999e+20f', 'missing_value':'9.999e+20f'}})\n",
    "    \n",
    "# Save the daily processed data\n",
    "out_basename2 = '{0}_{1}_{2}'.format(model_info, time_base, 'daily')\n",
    "out_file_daily = os.path.join(output_data_dir, out_basename2+'.nc')\n",
    "print('Writing daily forecast data to file: {0}'.format(out_file_daily))\n",
    "daily_precipitation.to_netcdf(out_file_daily, encoding={\"time\": {\"dtype\": \"float64\"}}) #, 'pr':{'_FillValue':'9.999e+20f', 'missing_value':'9.999e+20f'}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we will examine this data in ArcGIS Pro outside of the Notebook\n",
    "\n",
    "We will save this file for later and attempt to bias-correct it using archives of GFS forecasts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aprx.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine the archived GFS 0.25 degree data for India\n",
    "\n",
    "NCAR has put together an archive of GFS precipitation data over India. This archive of gridded precipiatation forecasts includes forecast lead time, which adds utility when performing some bias-correction routines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_NC = os.path.join(input_data_dir, \"archives\", \"gfs_india_0p25deg_24hr_20160101_to_20201231.nc\")\n",
    "data_var = 'Precipitation'\n",
    "time_coord = 'initialization_date'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explorre the gridded dataset using `xarray`\n",
    "\n",
    "Once we have identified a multidimensional dataset to analyze, we will open it using the [xarray](https://xarray.dev/) library. \n",
    "\n",
    "For a single input file, the `open_dataset` method will open a netCDF file. There are many input options, but the library should \n",
    "be able to auto-detect much of the necessary information from the input file. The result is a [DataSet](https://docs.xarray.dev/en/stable/generated/xarray.Dataset.html#xarray.Dataset) object, which \n",
    "holds the structure of the input dataset, any associated dimension information, coordinates, and metadata. This object can be used to access and manipulate the data for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the dataset as an xarray object\n",
    "ds = xr.open_dataset(input_NC)\n",
    "\n",
    "# Get total size of the dataset on disk\n",
    "dataset_size_GB = get_size_gb(ds)\n",
    "\n",
    "# Pull out the lead times represented in this forecast data\n",
    "lead_times = ds['lead'].values\n",
    "timesteps = ds[time_coord].values\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that xarray was very fast in opening this large dataset. That is because xarray does not need to read the data in order to return a DataSet object. It simply opens the data and reads the header information, which contains the self-describing metadata that gives the dimensions, coordinates, data variables and other attributes in the input file. \n",
    "Xarray will wait until we need to access the data variables before it attempts to read the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Choose a timestep with some significant precipitation\n",
    "ds[data_var].isel({'initialization_date':500, 'ensemble':0, 'lead':0}).plot(figsize=(10,8), cmap=cmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pick a point and examine the data at one point (1-dimensional forecast analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_lon_lat = (76.586, 12.43)    # KRS Dam\n",
    "print(f'Extracting point time-series for point with (longitude,latitude) = ({point_lon_lat})')\n",
    "\n",
    "# Use nearest neighbor method to extract the grid cell closest to the desired point\n",
    "ds_point = ds.sel(lat=point_lon_lat[1], lon=point_lon_lat[0], method=\"nearest\")\n",
    "print('Nearest grid point to {0} is ({1},{2})'.format(point_lon_lat, \n",
    "                                                      float(ds_point['lon'].data), \n",
    "                                                      float(ds_point['lat'].data)))\n",
    "ds_point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot time-series of forecasted precipication for one lead time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Plot a time series of precipitation for one lead time\n",
    "lead_time = 24\n",
    "ds_point['Precipitation'].sel(lead=lead_time).plot(figsize=(20,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot time series for all lead-times\n",
    "\n",
    "This plot will be a bit busy, but you can see how the precipitation time-series for various forecast lead times correlate. Further, there are some forecast lead times that exhibit more or less precipitation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Plot precipitation for this location with one line for each lead-time\n",
    "ds_point['Precipitation'].plot(figsize=(20,6), hue='lead')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the forecasts as a function of time using a sequential color scale\n",
    "\n",
    "Because we wish to plot time-series as a funciton of forecast lead time, it can be helpful to visually differentiate the lines using a sequential color ramp. Here we use the `autumn` color map and \n",
    "set a cycler to cycle through these colors when building the line plots. Further, you can temporally aggregate (or resample) the time series to limit the variations in the plotted data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create colormap\n",
    "#colors = plt.cm.viridis_r(np.linspace(0, 1, lead_times.shape[0]))\n",
    "colors = plt.cm.autumn(np.linspace(0, 1, lead_times.shape[0]))\n",
    "\n",
    "# Matplotlib options to set a specific color ramp for this plot\n",
    "ax = plt.subplot(111)\n",
    "fig = plt.gcf()\n",
    "ax.set_prop_cycle('color', list(colors))\n",
    "fig.set_size_inches(20,6)\n",
    "\n",
    "# Temporal manipulation for plotting purposes\n",
    "#ds_point['Precipitation'].resample({time_coord:\"1M\"}).mean().plot(ax=ax, hue='lead')\n",
    "ds_point['Precipitation'].rolling({time_coord:30}, center=True).mean().dropna(time_coord).plot(ax=ax, hue='lead')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check out active map\n",
    "\n",
    "Make sure that the map named **Precipitation** is your active map. You can confirm this by making sure the the map name is `Precipitation` in the Contents pane to the right. If not, then you can select the map named `Precipitation` from the open Maps above, and this will set that map as the active map. Or run the cell below to see which map is active."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get available information from the current map document\n",
    "aprx = arcpy.mp.ArcGISProject(\"current\")\n",
    "m = aprx.activeMap\n",
    "print('Active Map Name: {0}'.format(m.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will now create a Multidimensional Raster Layer to perform geoprocessing using arcpy\n",
    "\n",
    "Multidimensional Raster Layers are a way of representing data that contain multple dimensions. This data type behaves like a stack of individual rasters. We can create Multidimensional Raster Layers from netCDF data on disk. In this case, we will bring the GFS forecast archive into ArcGIS as a Multidimensional Raster Layer and then perform Zonal Statistics on the data against an area of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arcpy.md.MakeMultidimensionalRasterLayer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a multidimensional raster layer, specifying one lead time and one ensemble member"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Output layer name\n",
    "layer_name = \"GFS_India_Precip_Archive\"\n",
    "\n",
    "# Dimension selection\n",
    "Dimension_Values = \"lead 0;ensemble 1\"\n",
    "\n",
    "# Create a Multidimensional Rater Layer in the current active Map\n",
    "arcpy.md.MakeMultidimensionalRasterLayer(\n",
    "    in_multidimensional_raster=input_NC,\n",
    "    out_multidimensional_raster_layer=layer_name,\n",
    "    variables=data_var,\n",
    "    dimension_def=\"BY_VALUE\",\n",
    "    dimension_ranges=None,\n",
    "    dimension_values=Dimension_Values,\n",
    "    dimension=\"\",\n",
    "    start_of_first_iteration=\"\",\n",
    "    end_of_first_iteration=\"\",\n",
    "    iteration_step=None,\n",
    "    iteration_unit=\"\",\n",
    "    template='MAXOF',\n",
    "    dimensionless=\"DIMENSIONS\",\n",
    "    spatial_reference=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Spatial Aggregation using a basin polygon\n",
    "\n",
    "We will peform a powerful operation called `Zonal Statistics`, where a boundary is used to calculate statistics against a raster (multidmensional or not). Here, we will use a basin outline for the KRS reservoir, and extract a time series of statistics from the input precipitation dataset.\n",
    "\n",
    "* [How the zonal statistics tools work](https://pro.arcgis.com/en/pro-app/latest/tool-reference/spatial-analyst/how-zonal-statistics-works.htm)\n",
    "* [Zonal Statistics](https://pro.arcgis.com/en/pro-app/latest/tool-reference/spatial-analyst/zonal-statistics.htm)\n",
    "* [Zonal Statistics as Table](https://pro.arcgis.com/en/pro-app/latest/tool-reference/spatial-analyst/zonal-statistics-as-table.htm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Define the inputs and outputs\n",
    "in_polys = os.path.join(input_data_dir, 'basins', 'KRS_Basin.shp')\n",
    "out_table_name = \"ZonalStats_KRS\"\n",
    "Out_Table = os.path.join(default_gdb, out_table_name)\n",
    "\n",
    "# Run the Zonal Statistics as Table arcpy Spatial Analyst tool\n",
    "arcpy.sa.ZonalStatisticsAsTable(\n",
    "    in_zone_data=in_polys,\n",
    "    zone_field=\"BasinID\",\n",
    "    in_value_raster=layer_name,\n",
    "    out_table=Out_Table,\n",
    "    ignore_nodata=\"DATA\",\n",
    "    statistics_type=\"MEAN\",\n",
    "    process_as_multidimensional=\"ALL_SLICES\",\n",
    "    percentile_values=[90],\n",
    "    percentile_interpolation_type=\"AUTO_DETECT\",\n",
    "    circular_calculation=\"ARITHMETIC\",\n",
    "    circular_wrap_value=360\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the output has been added as a Standalone Table in the Contents pane of our active map. Let's examine the output table by right-clicking on the table and selecting `Open`.\n",
    "\n",
    "Notice that there is a row for each feature in the input polygons, as well as a row for each value in the dimensions of the Multidimensional Raster Layer tool. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Zonal Statistics table to pandas dataframe\n",
    "\n",
    "This will allow us to easily plot time series using the `pandas` library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List the fields you want to include. I want all columns except the geometry\n",
    "columns = [f.name for f in arcpy.ListFields(Out_Table) if f.type!=\"Geometry\"] \n",
    "df = pd.DataFrame(data=arcpy.da.SearchCursor(Out_Table, columns), columns=columns)\n",
    "df = df.set_index('StdTime')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['MEAN'].plot(figsize=(20,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing ArcPy Chart objects\n",
    "\n",
    "If you work with [Chart objects](https://pro.arcgis.com/en/pro-app/latest/arcpy/classes/chart.htm) in ArcPy, you can visualize these with rich notebook representations (new in ArcGIS Pro 2.6):\n",
    "\n",
    "#### Plot a bar chart of zonal average precipitation\n",
    "\n",
    "https://pro.arcgis.com/en/pro-app/latest/arcpy/charts/bar.htm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "c = arcpy.charts.Bar(x='StdTime', y='MEAN',\n",
    "                     title=\"Basin average precipitation\",\n",
    "                     xTitle=\"Time\", yTitle=\"Mean Basin Precipitation\",\n",
    "                     dataSource=Out_Table)\n",
    "\n",
    "# You can add this chart to an existing layer\n",
    "tableLayer = m.listTables(out_table_name)[0]\n",
    "c.addToLayer(tableLayer)\n",
    "\n",
    "# Display the chart in the notebook, here\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will see the chart populate under the `ZonalStats_KRS` table in the Contents pane. Right-click to open the chart and you can play with some of the chart capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aprx.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reset the namespace\n",
    "\n",
    "The following `%reset -f` command is a built-in command in Jupyter Notebook that will reset the namespace. This is good practice to run when you are finished with the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next up - Bias Correcting a Gridded Precipitation Forecast\n",
    "\n",
    "This concludes this lesson. In the next lesson we will look at extending bias-correction methods from 1D to 2-dimensions.\n",
    "\n",
    "**IT IS BEST TO EITHER SHUTDOWN THIS LESSON OR CLOSE IT BEFORE PROCEEDING TO THE NEXT LESSON TO AVOID POSSIBLY EXCEEDING ALLOCATED MEMORY. Select `Command Pallette -> restart kernel`.**\n",
    "\n",
    "© UCAR 2023"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ArcGISPro",
   "language": "Python",
   "name": "python3"
  },
  "language_info": {
   "file_extension": ".py",
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
