{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "362dfd50",
   "metadata": {},
   "source": [
    "# Bias Correct GFS Precipitation Forecast\n",
    "\n",
    "In this exercise, we will bias correct a previously-obtained GFS forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe28904",
   "metadata": {},
   "source": [
    "### Import modules and set configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb25caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the directory of the current project and add to PATH\n",
    "import sys, os, arcpy\n",
    "home_folder = arcpy.mp.ArcGISProject(\"current\").homeFolder\n",
    "sys.path.insert(0, home_folder)\n",
    "os.chdir(home_folder)\n",
    "\n",
    "# The 00_environment_setup notebook contains libraries and other things common to all the notebooks (e.g. file paths)\n",
    "%run \"00_environment_setup.ipynb\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc657139",
   "metadata": {},
   "source": [
    "We can add the scripts directory to our PATH environment and import the python scripts as if they were modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593dd4b1-a7e1-4d7a-b7cd-e542b256b2cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# To import scripts from the \\scripts directory, we need to add a relative path that will be 2 directories above this Notebook\n",
    "sys.path.insert(0, os.path.abspath('..\\..\\scripts'))\n",
    "\n",
    "# Import local scripts from the \\scripts directory\n",
    "use_cartopy=False\n",
    "import forecast_preprocess\n",
    "import definitions\n",
    "import read_input_file_xarray\n",
    "if use_cartopy:\n",
    "    import plotting_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b630b2e2-fad0-4d33-a8e8-55d15d2c8b30",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Set configuration parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b3aa9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "regridding=False\n",
    "\n",
    "#If true, then diagnostic plots will be created\n",
    "diagnostic_plots=True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b1d1a8-e31f-4bfc-bcf3-1d4c7217d1fd",
   "metadata": {},
   "source": [
    "Other configuration details for the datasets and grids used are in definitions.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a99a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Date to run (sample data, for now.  Will be realtime)\n",
    "date=datetime(2023,10,18,0,0)\n",
    "\n",
    "#Observations to use as reference\n",
    "obs='IMDobs'\n",
    "\n",
    "#Realtime input forecast model, details about the input files from this model are given in the definitions.py file\n",
    "model='gfs' \n",
    "realtime_acc=3\n",
    "archive_acc=24\n",
    "acc_str='hr'\n",
    "\n",
    "#Common grid to be used in analysis (defined in definitions.py file)\n",
    "grid = 'india_0p25deg'\n",
    "\n",
    "# Forecast start and end times\n",
    "fcst_archive_start=datetime(2016,1,1)\n",
    "fcst_archive_end=datetime(2020,12,31)\n",
    "\n",
    "# Observation start and end times\n",
    "obs_archive_start=datetime(2016,1,1)\n",
    "obs_archive_end=datetime(2019,12,31)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f18472a",
   "metadata": {},
   "source": [
    "### Construct the expected archive filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf694c8-2ed7-40fc-9afd-0a61656aba81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Observation Archive\n",
    "obs_filename = '{0}_{1}_{2}{3}_{4}_to_{5}.nc'.format(obs, \n",
    "                                                     grid, \n",
    "                                                     archive_acc, \n",
    "                                                     acc_str, \n",
    "                                                     obs_archive_start.strftime('%Y%m%d'), \n",
    "                                                     obs_archive_end.strftime('%Y%m%d'))\n",
    "obs_archive_file = os.path.join(input_data_dir, 'archives', obs_filename)\n",
    "\n",
    "# Forecast Archive\n",
    "forecast_filename = '{0}_{1}_{2}{3}_{4}_to_{5}.nc'.format(model, \n",
    "                                                     grid, \n",
    "                                                     archive_acc, \n",
    "                                                     acc_str, \n",
    "                                                     fcst_archive_start.strftime('%Y%m%d'), \n",
    "                                                     fcst_archive_end.strftime('%Y%m%d'))\n",
    "forecast_archive_file = os.path.join(input_data_dir, 'archives', forecast_filename)\n",
    "\n",
    "# Forecast Realtime File\n",
    "if regridding==False:\n",
    "    #If we already have a realtime forecast on the correct grid, then we don't need regridding\n",
    "    # and we will specify this realtime forecast file name.      \n",
    "    forecast_rt_filename = '{0}_0p25_1hr_{1}z_{2}{3}_daily.nc'.format(model,\n",
    "                                                                            date.strftime('%H'),\n",
    "                                                                            model,\n",
    "                                                                            date.strftime('%Y%m%d'))\n",
    "    forecast_realtime_file = os.path.join(output_data_dir, forecast_rt_filename)\n",
    "                                          \n",
    "elif regridding==True:\n",
    "    #Otherwise, we will be using raw forecasts downloaded from the web and these will processed to netcdf on an appropriate grid.\n",
    "    #In this case, reading the forecasts will be done using the the sub-routines in read_input_files_xarray.py, using the key below\n",
    "    realtime_modelkey=model+'_'+str(realtime_acc)+acc_str \n",
    "\n",
    "diag_plot_dir = os.path.join(output_data_dir, 'diagnostic_plots', model)\n",
    "if diagnostic_plots:\n",
    "    if not os.path.exists(diag_plot_dir):\n",
    "        os.makedirs(diag_plot_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7d807d",
   "metadata": {},
   "source": [
    "### Open the archived observation dataset as a `xarray` DataSet object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c337fc-a52a-4e45-b4ce-5adca0b8bccc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(obs_archive_file)\n",
    "ds_obs = xr.open_dataset(obs_archive_file)\n",
    "ds_obs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fef176",
   "metadata": {},
   "source": [
    "### Manipulate the archived observation dataset and plot over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d151709-e00a-443f-af3f-6f50dfb45c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%matplotlib inline\n",
    "\n",
    "# Compute the mean over latitude and longitude dimensions, then plot\n",
    "ds_obs['Precipitation'].mean(('lat','lon')).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6cf41f-e74c-479e-9521-9aad6208081f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%matplotlib inline\n",
    "\n",
    "# Compute mean over latitude and longitude dimensions and select dates in year 2018, then plot\n",
    "ds_obs['Precipitation'].mean(('lat','lon')).sel(start_date=ds_obs.start_date.dt.year.isin([2018])).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b24e6d2",
   "metadata": {},
   "source": [
    "### Open the archived forecast dataset as a `xarray` DataSet object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e4944c-8605-4747-8929-42430a4c6f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(forecast_archive_file)\n",
    "ds_fcst = xr.open_dataset(forecast_archive_file)\n",
    "ds_fcst"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c827521a",
   "metadata": {},
   "source": [
    "### Manipulate the archived forecast dataset and plot over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5495f8-e64f-4a78-beed-614bf428841b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%matplotlib inline\n",
    "\n",
    "# Compute the mean over latitude and longitude dimensions, then plot\n",
    "ds_fcst['Precipitation'].mean(('lat', 'lon')).mean('initialization_date').plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9e6cc4",
   "metadata": {},
   "source": [
    "### Plot a sample location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2217099",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lat=17.75\n",
    "test_lon=78.25\n",
    "obs_sorted=np.sort(ds_obs.Precipitation.sel(lat=test_lat).sel(lon=test_lon).dropna(dim='start_date').values)\n",
    "fcst_sorted=np.sort(ds_fcst.Precipitation.sel(lat=17.75).sel(lon=78.25)[:,0,5].dropna(dim='initialization_date').values)\n",
    "x_fcst_sorted=np.arange(0,1,1/len(fcst_sorted))\n",
    "x_obs_sorted=np.arange(0,1,1/len(obs_sorted))\n",
    "fcst_line=plt.plot(x_fcst_sorted,fcst_sorted)\n",
    "\n",
    "#Restrict range of y axis to explore behavior at lower rainfall amounts, if desired\n",
    "#plt.ylim((0,2))\n",
    "#plt.ylim((0,2))\n",
    "obs_line=plt.plot(x_obs_sorted,obs_sorted)\n",
    "plt.legend(['Fcst','Obs'])\n",
    "plt.ylabel(\"Rainfall, mm/hour\")\n",
    "plt.xlabel(\"Probability\")\n",
    "plt.title(\"Sorted Archived Rainfall values for (\"+str(test_lat)+\" ,\"+str(test_lon)+\")\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f8a4cf-7344-4556-b2ba-ace1fc20d197",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create input for regridding if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e838044",
   "metadata": {},
   "source": [
    "Setup regridding dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942da5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "if regridding==True:\n",
    "    regrid_in={}\n",
    "\n",
    "    #Use conservative for going from a fine to course grid #Use biliear for going from a course to fine grid\n",
    "    regrid_in['regrid_type'] = 'conservative'\n",
    "    regrid_in['forecast_key'] = realtime_modelkey\n",
    "\n",
    "    #input from Kevin's NOMADS GFS code\n",
    "    regrid_in['input_filetype']='daily_netcdf'\n",
    "\n",
    "    regrid_in['model'] = model\n",
    "    regrid_in['output_grid'] = grid\n",
    "\n",
    "    #Input data is 3 hour averages\n",
    "    regrid_in['input_acc'] = realtime_acc\n",
    "\n",
    "    #Output data is 24 hour averages\n",
    "    regrid_in['output_acc'] = archive_acc\n",
    "\n",
    "    # Outputs\n",
    "    regrid_in['out_data_dir'] = os.path.join(output_data_dir, 'realtime_sample_data', model, 'regridded', date.strftime('%Y'))\n",
    "    regrid_in['out_data_fn'] = os.path.join(regrid_in['out_data_dir'], '{0}_{1}_{2}{3}_{4}.nc'.format(model, grid, archive_acc, acc_str, date.strftime('%Y%m%d%H')))\n",
    "    regrid_in['diagnostics'] = diagnostic_plots\n",
    "    regrid_in['diag_plots_dir'] = diag_plot_dir\n",
    "    regrid_in['use_cartopy'] = use_cartopy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9228f7",
   "metadata": {},
   "source": [
    "## Run regridding if needed, otherwise read forecast file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e63564",
   "metadata": {},
   "outputs": [],
   "source": [
    "if regridding==True:\n",
    "    #Read in raw downloaded NOMADS data and put them in a netcdf file in the format needed for multi-dimensional bias-corretion\n",
    "    ds_out = forecast_preprocess.regrid_forecast(date,regrid_in)\n",
    "    \n",
    "elif regridding==False:\n",
    "    #Read in netcdf file, and put in format needed for multi-dimensional bias-correction\n",
    "    ds=xr.open_dataset(forecast_realtime_file)\n",
    "    ds_out=xr.Dataset(data_vars=dict(Precipitation=([\"ens\",\"lead\",\"lat\",\"lon\"],ds.pr.transpose(\"time\",\"lat\",\"lon\").expand_dims(dim={\"ens\":1}).values),\\\n",
    "                        start_date=([\"lead\"],ds.time.values)),\\\n",
    "                        coords=dict(ens=(\"ens\",[0]),lead=(\"lead\",np.arange(0,240,24)),lat=(\"lat\",ds.lat.values),\\\n",
    "                        lon=(\"lon\",ds.lon.values)))\n",
    "    if diagnostic_plots:\n",
    "        if use_cartopy:\n",
    "            plotting_utils.basic_map_gridplot(ds_out.Precipitation[0,5,:,:],diag_plots_dir+'regridded_'+model+'_'+date.strftime('%Y%m%d%H')+'_lead5.png',10)\n",
    "        else: \n",
    "            #CHANGED SEGMENT BELOW to add new color bar and scaling, as well as mask out non-india points to match other plot\n",
    "            india_mask=(ds_obs.Precipitation[0,:,:]+1)/(ds_obs.Precipitation[0,:,:]+1)\n",
    "            values_to_plot=ds_out.Precipitation[0,5,:,:]*india_mask\n",
    "            cmap = matplotlib.colormaps['gist_ncar']  # Color ramp suitable for precipiation\n",
    "            cmap.set_under('lightgrey')               # Set values of zero precipitation to grey\n",
    "            values_to_plot.plot(cmap=cmap,vmin=0.001,vmax=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3b34ab-75e2-4526-8beb-22b3244560fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a7d6d1",
   "metadata": {},
   "source": [
    "## Create input for quantile mapping\n",
    "\n",
    "Setup Q2Q dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c668b159",
   "metadata": {},
   "outputs": [],
   "source": [
    "q2q_in={}\n",
    "q2q_in['obs_archive_file']=obs_archive_file\n",
    "q2q_in['forecast_archive_file']=forecast_archive_file\n",
    "\n",
    "#Perform bias correction only on same hour of day\n",
    "q2q_in['diurnal_cycle_filter']=False \n",
    "\n",
    "#Perform bias correction only on same month of the year\n",
    "q2q_in['seasonal_cycle_filter']=False\n",
    "\n",
    "# Outputs\n",
    "q2q_in['out_data_dir']=os.path.join(output_data_dir, \n",
    "                                    'realtime_sample_data', \n",
    "                                    model, \n",
    "                                    'q2q_corrected')\n",
    "q2q_in['out_data_fn']=os.path.join(q2q_in['out_data_dir'], \n",
    "                                   '{0}_{1}_{2}{3}_{4}.nc'.format(model, \n",
    "                                                                  grid, \n",
    "                                                                  archive_acc, \n",
    "                                                                  acc_str, \n",
    "                                                                  date.strftime('%Y%m%d%H')))    \n",
    "    \n",
    "# Other options\n",
    "q2q_in['diagnostics']=diagnostic_plots\n",
    "q2q_in['diag_plots_dir']=diag_plot_dir\n",
    "q2q_in['date']=date\n",
    "q2q_in['acc_period_str']=str(archive_acc)+acc_str\n",
    "q2q_in['forecast_model']=model #'wrf_belize','gfs_ncar_ds85p1','gfs_hires_3hrly','cmc'\n",
    "q2q_in['use_cartopy']=use_cartopy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0ee801",
   "metadata": {},
   "source": [
    "## Run quantile mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfc20a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "pcp_adjusted=forecast_preprocess.bias_correct_forecast(q2q_in,ds_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c64961",
   "metadata": {},
   "outputs": [],
   "source": [
    "pcp_adjusted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a36c62",
   "metadata": {},
   "source": [
    "### Reset the namespace\n",
    "\n",
    "The following `%reset -f` command is a built-in command in Jupyter Notebook that will reset the namespace. This is good practice to run when you are finished with the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db68100",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ArcGISPro",
   "language": "Python",
   "name": "python3"
  },
  "language_info": {
   "file_extension": ".py",
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
