{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Gridded Precipitation Datasets\n",
    "\n",
    "In this lesson, we will use data obtained in previous lessons. \n",
    "\n",
    "1. Gridded GFS precipication forecast data, aggregated to daily precipiation totals.\n",
    "2. Bias-corrected GFS precipitation forecast data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the directory of the current project and add to PATH\n",
    "import sys, os, arcpy\n",
    "from glob import glob\n",
    "\n",
    "# The 00_environment_setup notebook contains libraries and other things common to all the notebooks (e.g. file paths)\n",
    "%run \"00_environment_setup.ipynb\"\n",
    "\n",
    "# Additional libraries needed for this script\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Precipitation color-maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = matplotlib.colormaps['gist_ncar']  # Color ramp suitable for precipiation\n",
    "cmap.set_under('lightgrey')               # Set values of zero precipitation to grey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define input datasets\n",
    "\n",
    "This exercise requires data that was computed during previous exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basin polygon for peforming spatial aggregation\n",
    "basin = os.path.join(input_data_dir, 'basins', 'KRS_Basins.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the dataset either by name or using a wildcard string and the glob function\n",
    "GFS_daily = glob(os.path.join(output_data_dir, 'gfs_0p25_1hr_*_daily.nc'))[-1]\n",
    "print('Found daily GFS file:\\n\\t{0}'.format(GFS_daily))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the dataset either by name or using a wildcard string and the glob function\n",
    "GFS_Bias_Corrected = glob(os.path.join(output_data_dir, 'realtime_sample_data', '**/*.nc'), recursive = True)[0]\n",
    "print('Found bias-corrected daily GFS file:\\n\\t{0}'.format(GFS_Bias_Corrected))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open the datasets using xarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GFS_input_ds = xr.open_dataset(GFS_daily)\n",
    "GFS_input_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the GFS forecast for the first time-step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline            \n",
    "GFS_input_ds['pr'].isel({'time':0}).plot(figsize=(10,8), cmap=cmap,vmin=0.001,vmax=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GFS_corrected_ds = xr.open_dataset(GFS_Bias_Corrected)\n",
    "GFS_corrected_ds = GFS_corrected_ds.rename({'Precipitation':'pr_corrected', 'forecast_time': 'time'})\n",
    "GFS_corrected_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the bias-corrected GFS forecast for the first time-step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "GFS_corrected_ds['pr_corrected'].isel({'time':0}).plot(figsize=(10,8), cmap=cmap,vmin=0.001,vmax=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct a valid-data mask so we do not compare NoData cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "india_mask = (GFS_corrected_ds['pr_corrected'][0,:,:]+1)/(GFS_corrected_ds['pr_corrected'][0,:,:]+1)\n",
    "india_mask.plot(figsize=(10,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the mask to the GFS forecast data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline    \n",
    "GFS_input_ds['pr']*=india_mask        \n",
    "GFS_input_ds['pr'].isel({'time':0}).plot(figsize=(10,8), cmap=cmap,vmin=0.001,vmax=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract a time-series from a single point within the GFS domains\n",
    "\n",
    "We will select one grid-cell from within the model subset domain and plot the data for that cell over time. \n",
    "\n",
    "Again, `xarray` will know how to plot the data based on the number of dimensions (in this case just `time`), and plot the data with a line, labeling the axes with the coordinate values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Select a location in the model's x,y coordinate system.\n",
    "#point_lon_lat = (76.586, 12.43)    # KRS Dam\n",
    "point_lon_lat = (71.25, 27.75)      # More interesting location in test_data\n",
    "print(f'Extracting point time-series for point with (longitude,latitude) = ({point_lon_lat})')\n",
    "\n",
    "# Use nearest neighbor method to extract the grid cell closest to the desired point\n",
    "ds_point_1 = GFS_input_ds.sel(lat=point_lon_lat[1], lon=point_lon_lat[0], method=\"nearest\")\n",
    "ds_point_2 = GFS_corrected_ds.sel(lat=point_lon_lat[1], lon=point_lon_lat[0], method=\"nearest\")\n",
    "\n",
    "# Plot the time-series\n",
    "fig, ax = plt.subplots(figsize=(20,6)) \n",
    "ds_point_1['pr'].plot(ax=ax, label='GFS Precipitation')\n",
    "ds_point_2['pr_corrected'].plot(ax=ax, label='Corrected Precipitation')\n",
    "ax.legend(loc=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot GFS against Bias-Corrected GFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge datasets together for plotting\n",
    "plot_ds = xr.merge([GFS_input_ds, GFS_corrected_ds])\n",
    "\n",
    "# Initiate the plot\n",
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "\n",
    "# Plot a 1:1 line\n",
    "max_val = np.maximum(plot_ds.max()['pr'], plot_ds.max()['pr_corrected'])\n",
    "plt.plot([0,max_val.data], [0,max_val.data], 'k-')\n",
    "\n",
    "# Plot the data as a scatterplot\n",
    "plot_ds.plot.scatter(ax=ax, x=\"pr_corrected\", y=\"pr\", s=4, color=\"red\", marker=\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Spatial Aggregation using Zonal Statistics\n",
    "\n",
    "In this section, we will perform spatial aggregations using a basin boundary upsgream of the K.R.S Reservoir. In order to do this we must:\n",
    "\n",
    "1. Create individual Multidimenional Raster Layers from the input netCDF files created earlier.\n",
    "2. Use `Zonal Statistics as Table` to perform a multidimensional spatial aggregation.\n",
    "3. Plot the resulting time-series.\n",
    "\n",
    "#### Build a Multidimensional Raster Layer for each forecast product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Output layer name\n",
    "layer_name1 = \"GFS_Forecast\"\n",
    "\n",
    "# Create a Multidimensional Rater Layer in the current active Map\n",
    "arcpy.md.MakeMultidimensionalRasterLayer(\n",
    "    in_multidimensional_raster=GFS_daily,\n",
    "    out_multidimensional_raster_layer=layer_name1,\n",
    "    variables=\"pr\",\n",
    "    dimension_def=\"ALL\",\n",
    "    dimension_ranges=None,\n",
    "    dimension_values=None,\n",
    "    dimension=\"\",\n",
    "    start_of_first_iteration=\"\",\n",
    "    end_of_first_iteration=\"\",\n",
    "    iteration_step=None,\n",
    "    iteration_unit=\"\",\n",
    "    dimensionless=\"DIMENSIONS\",\n",
    "    spatial_reference=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output layer name\n",
    "layer_name2 = \"GFS_Forecast_Bias_Corrected\"\n",
    "\n",
    "# Create a Multidimensional Rater Layer in the current active Map\n",
    "arcpy.md.MakeMultidimensionalRasterLayer(\n",
    "    in_multidimensional_raster=GFS_Bias_Corrected,\n",
    "    out_multidimensional_raster_layer=layer_name2,\n",
    "    variables='Precipitation',\n",
    "    dimension_def=\"ALL\",\n",
    "    dimension_ranges=None,\n",
    "    dimension_values=None,\n",
    "    dimension=\"\",\n",
    "    start_of_first_iteration=\"\",\n",
    "    end_of_first_iteration=\"\",\n",
    "    iteration_step=None,\n",
    "    iteration_unit=\"\",\n",
    "    dimensionless=\"DIMENSIONS\",\n",
    "    spatial_reference=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Spatial Aggregation using Zonal Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, set the input layer\n",
    "in_polys = os.path.join(input_data_dir, 'basins', 'KRS_Basin.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Define the outputs\n",
    "out_table_name1 = \"ZonalStats_KRS_GFS_Daily\"\n",
    "Out_Table1 = os.path.join(default_gdb, out_table_name1)\n",
    "\n",
    "# Run the Zonal Statistics as Table arcpy Spatial Analyst tool\n",
    "arcpy.sa.ZonalStatisticsAsTable(\n",
    "    in_zone_data=in_polys,\n",
    "    zone_field=\"BasinID\",\n",
    "    in_value_raster=layer_name1,\n",
    "    out_table=Out_Table1,\n",
    "    ignore_nodata=\"DATA\",\n",
    "    statistics_type=\"MEAN\",\n",
    "    process_as_multidimensional=\"ALL_SLICES\",\n",
    "    percentile_values=[90],\n",
    "    percentile_interpolation_type=\"AUTO_DETECT\",\n",
    "    circular_calculation=\"ARITHMETIC\",\n",
    "    circular_wrap_value=360\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Define the outputs\n",
    "out_table_name2 = \"ZonalStats_KRS_GFS_Daily_Bias_Corrected\"\n",
    "Out_Table2 = os.path.join(default_gdb, out_table_name2)\n",
    "\n",
    "# Run the Zonal Statistics as Table arcpy Spatial Analyst tool\n",
    "arcpy.sa.ZonalStatisticsAsTable(\n",
    "    in_zone_data=in_polys,\n",
    "    zone_field=\"BasinID\",\n",
    "    in_value_raster=layer_name2,\n",
    "    out_table=Out_Table2,\n",
    "    ignore_nodata=\"DATA\",\n",
    "    statistics_type=\"MEAN\",\n",
    "    process_as_multidimensional=\"ALL_SLICES\",\n",
    "    percentile_values=[90],\n",
    "    percentile_interpolation_type=\"AUTO_DETECT\",\n",
    "    circular_calculation=\"ARITHMETIC\",\n",
    "    circular_wrap_value=360\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the resulting tables have been added to the Contents pane, you can create charts with the table data using the graphical user interface (GUI). Or you can proceed to create plots in the notebook using the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List the fields you want to include.\n",
    "columns1 = [f.name for f in arcpy.ListFields(Out_Table1) if f.type!=\"Geometry\"] \n",
    "df1 = pd.DataFrame(data=arcpy.da.SearchCursor(Out_Table1, columns1), columns=columns1)\n",
    "df1 = df1.set_index('StdTime')\n",
    "\n",
    "columns2 = [f.name for f in arcpy.ListFields(Out_Table2) if f.type!=\"Geometry\"] \n",
    "df2 = pd.DataFrame(data=arcpy.da.SearchCursor(Out_Table2, columns2), columns=columns2)\n",
    "df2['StdTime'] = pd.to_datetime(df2['StdTime'])\n",
    "df2 = df2.set_index('StdTime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['MEAN'].plot(figsize=(20,8))\n",
    "df2['MEAN'].plot(figsize=(20,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['MEAN'] - df1['MEAN']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bring in the reservoir inflow forecast\n",
    "\n",
    "Now tie together the precipitation forecast for the basin and the GEOGloWS reservoir inflow forecast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#res_inflow_stats = os.path.join(output_data_dir, 'GeoGLOWS_Reservoir_Inflow_ensemble_stats_K_R_Sagara.csv')\n",
    "res_inflow_stats = os.path.join(input_data_dir, 'geoglows', 'GeoGLOWS_Reservoir_Inflow_ensemble_stats_K_R_Sagara.csv')\n",
    "res_inflow_df = pd.read_csv(res_inflow_stats)\n",
    "res_inflow_df['datetime'] = pd.to_datetime(res_inflow_df['datetime'])\n",
    "res_inflow_df = res_inflow_df.set_index('datetime')\n",
    "res_inflow_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup a plot with 3 axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the plot\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "ax.set_ylabel('Precipitation total (mm/day)')\n",
    "\n",
    "# Add a second axis with same x\n",
    "ax2 = ax.twinx()\n",
    "ax2.set_ylabel('Streamflow (m^3/s)')\n",
    "ax2.set_frame_on(True)\n",
    "ax2.patch.set_visible(False)\n",
    "\n",
    "# Plot each data series\n",
    "df1['MEAN'].plot(ax=ax, style='b-')\n",
    "df2['MEAN'].plot(ax=ax, style='r-')\n",
    "res_inflow_df['flow_avg_m^3/s'].plot(ax=ax2, style='g-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aprx.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reset the namespace\n",
    "\n",
    "The following `%reset -f` command is a built-in command in Jupyter Notebook that will reset the namespace. This is good practice to run when you are finished with the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next up - Optional Lessons\n",
    "\n",
    "This concludes the lesson.\n",
    "\n",
    "**IT IS BEST TO EITHER SHUTDOWN THIS LESSON OR CLOSE IT BEFORE PROCEEDING TO THE NEXT LESSON TO AVOID POSSIBLY EXCEEDING ALLOCATED MEMORY. Select `Command Pallette -> restart kernel`.**\n",
    "\n",
    "Â© UCAR 2023"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ArcGISPro",
   "language": "Python",
   "name": "python3"
  },
  "language_info": {
   "file_extension": ".py",
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
